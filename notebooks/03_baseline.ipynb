{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f4949f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-engineered training data loaded successfully\n",
      "Shape: (3043, 40)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "\n",
    "FEATURE_PATH = '../'\n",
    "full_train_df = pd.read_csv(os.path.join(FEATURE_PATH, 'train_features.csv'))\n",
    "print(\"Feature-engineered training data loaded successfully\")\n",
    "print(f\"Shape: {full_train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ec2a655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared for modeling:\n",
      "Features shape: (3043, 36)\n",
      "Target shape: (3043,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for modeling\n",
    "features = [col for col in full_train_df.columns if col not in ['object_id', 'target', 'split', 'English Translation']]\n",
    "\n",
    "X = full_train_df[features]\n",
    "y = full_train_df['target']\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "X = pd.DataFrame(X, columns=features)\n",
    "\n",
    "print(\"Data prepared for modeling:\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training with 5-Fold Stratified Cross-Validation...\n",
      "--- Fold 1/5 ---\n",
      "F1 Score for Fold 1: 0.1852\n",
      "--- Fold 2/5 ---\n",
      "F1 Score for Fold 2: 0.3137\n",
      "--- Fold 3/5 ---\n",
      "F1 Score for Fold 3: 0.4082\n",
      "--- Fold 4/5 ---\n",
      "F1 Score for Fold 4: 0.3913\n",
      "--- Fold 5/5 ---\n",
      "F1 Score for Fold 5: 0.1600\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "Mean F1 Score across all folds: 0.2917\n"
     ]
    }
   ],
   "source": [
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds = np.zeros(len(full_train_df))\n",
    "f1_scores = []\n",
    "\n",
    "print(f\"\\nStarting training with {N_SPLITS}-Fold Stratified Cross-Validation...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "\n",
    "    # Split the data\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "    # LightGBM Model\n",
    "    neg_count = y_train.value_counts()[0]\n",
    "    pos_count = y_train.value_counts()[1]\n",
    "    scale_pos_weight_value = neg_count / pos_count if pos_count > 0 else 1\n",
    "    \n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'seed': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'subsample': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'scale_pos_weight': scale_pos_weight_value\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              eval_metric='f1',\n",
    "              callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    val_preds_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    val_preds_binary = (val_preds_proba > 0.5).astype(int)\n",
    "    \n",
    "    f1 = f1_score(y_val, val_preds_binary)\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"F1 Score for Fold {fold+1}: {f1:.4f}\")\n",
    "\n",
    "# Final Results\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "print(f\"Mean F1 Score across all folds: {mean_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4e1f6",
   "metadata": {},
   "source": [
    "Mean F1 Score of 0.2917 is the starting point. The F1 score fluctuated considerably between folds (from a low of 0.16 to a high of 0.41). This is happening because we are dealing with imbalanced datasets where the small number of positive samples can lead to instability depending on how the data is split.\n",
    "\n",
    "Next step would be to find the optimal probability threshold that maximizes the F1 score for each fold."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
