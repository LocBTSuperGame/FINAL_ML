{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d9b27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "\n",
    "# Suppress pandas warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47df7092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Data Loading\n",
      "All data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1. Configuration\n",
    "DATA_PATH = '../data/'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "print(\"Data Loading\")\n",
    "try:\n",
    "    train_log_df = pd.read_csv(os.path.join(DATA_PATH, 'train_log.csv'))\n",
    "    metadata_full = train_log_df[['object_id', 'Z', 'EBV', 'target']].copy()\n",
    "\n",
    "    all_lc_df_list = []\n",
    "    for split_folder in train_log_df['split'].unique():\n",
    "        path = os.path.join(DATA_PATH, split_folder, 'train_full_lightcurves.csv')\n",
    "        all_lc_df_list.append(pd.read_csv(path))\n",
    "    full_lc_df = pd.concat(all_lc_df_list).dropna()\n",
    "    print(\"All data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please ensure your DATA_PATH is set correctly.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3011ec19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Lightcurves: 100%|██████████| 3043/3043 [00:03<00:00, 890.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing complete.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_lightcurves(df):\n",
    "    processed_dfs = []\n",
    "    for object_id, group in tqdm(df.groupby('object_id'), desc=\"Processing Lightcurves\"):\n",
    "        group = group.copy()\n",
    "        \n",
    "        # Scale Flux and Flux_err to focus on shape, not magnitude\n",
    "        scaler = StandardScaler()\n",
    "        group[['Flux', 'Flux_err']] = scaler.fit_transform(group[['Flux', 'Flux_err']])\n",
    "        \n",
    "        group['Time (MJD)'] = group['Time (MJD)'] - group['Time (MJD)'].min()\n",
    "        \n",
    "        processed_dfs.append(group)\n",
    "        \n",
    "    return pd.concat(processed_dfs)\n",
    "\n",
    "processed_lc_df = preprocess_lightcurves(full_lc_df)\n",
    "\n",
    "# One-hot encode filters after all other processing\n",
    "processed_lc_df = pd.get_dummies(processed_lc_df, columns=['Filter'])\n",
    "\n",
    "# Group the final processed data for fast lookup in the Dataset\n",
    "grouped_lc = processed_lc_df.groupby('object_id')\n",
    "\n",
    "# Scale static metadata features\n",
    "scaler_static = StandardScaler()\n",
    "metadata_full[['Z', 'EBV']] = scaler_static.fit_transform(metadata_full[['Z', 'EBV']])\n",
    "print(\"Pre-processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d519154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Dataset and DataLoader ---\n",
      "Dataset and DataLoaders are ready.\n"
     ]
    }
   ],
   "source": [
    "# 4. PyTorch Dataset and DataLoader\n",
    "print(\"PyTorch Dataset and DataLoader ---\")\n",
    "class MALLORNDataset(Dataset):\n",
    "    def __init__(self, metadata, grouped_lc):\n",
    "        self.metadata = metadata\n",
    "        self.grouped_lc = grouped_lc\n",
    "        self.object_ids = metadata['object_id'].tolist()\n",
    "        # Define all possible filter columns after one-hot encoding\n",
    "        self.all_filter_cols = ['Filter_g', 'Filter_i', 'Filter_r', 'Filter_u', 'Filter_y', 'Filter_z']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.object_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        object_id = self.object_ids[idx]\n",
    "        \n",
    "        # Get pre-processed data\n",
    "        lc_data = self.grouped_lc.get_group(object_id)\n",
    "        meta_row = self.metadata[self.metadata['object_id'] == object_id]\n",
    "\n",
    "        # Ensure all filter columns exist, filling missing ones with 0\n",
    "        for col in self.all_filter_cols:\n",
    "            if col not in lc_data.columns:\n",
    "                lc_data[col] = 0\n",
    "        \n",
    "        feature_cols = ['Time (MJD)', 'Flux', 'Flux_err'] + self.all_filter_cols\n",
    "        features = lc_data[feature_cols].astype(np.float32)\n",
    "\n",
    "        # Static features and target\n",
    "        static_features = meta_row[['Z', 'EBV']].astype(np.float32).values.flatten()\n",
    "        target = float(meta_row['target'].values[0])\n",
    "\n",
    "        return {\n",
    "            'sequence': torch.tensor(features.values, dtype=torch.float32),\n",
    "            'static': torch.tensor(static_features, dtype=torch.float32),\n",
    "            'target': torch.tensor(target, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences = [item['sequence'] for item in batch]\n",
    "    statics = torch.stack([item['static'] for item in batch])\n",
    "    targets = torch.stack([item['target'] for item in batch])\n",
    "    \n",
    "    padded_sequences = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "    \n",
    "    return {\n",
    "        'sequence': padded_sequences,\n",
    "        'static': statics,\n",
    "        'target': targets.unsqueeze(1)\n",
    "    }\n",
    "\n",
    "# Stratified split to maintain class balance\n",
    "train_meta, val_meta = train_test_split(\n",
    "    metadata_full, test_size=0.2, random_state=42, stratify=metadata_full['target']\n",
    ")\n",
    "\n",
    "train_dataset = MALLORNDataset(train_meta, grouped_lc)\n",
    "val_dataset = MALLORNDataset(val_meta, grouped_lc)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "print(\"Dataset and DataLoaders are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77015ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture\n",
      "Model defined.\n"
     ]
    }
   ],
   "source": [
    "# 5. Model Architecture (GRU + Attention)\n",
    "print(\"Model Architecture\")\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        energy = torch.tanh(self.attn(hidden_states))\n",
    "        attention_scores = self.v(energy).squeeze(-1)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "        context_vector = torch.bmm(attention_weights.unsqueeze(1), hidden_states).squeeze(1)\n",
    "        return context_vector\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_size, static_size, hidden_size, num_layers, dropout):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True,\n",
    "                          bidirectional=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.attention = Attention(hidden_size * 2)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2 + static_size, 256), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1))\n",
    "\n",
    "    def forward(self, seq, static):\n",
    "        gru_out, _ = self.gru(seq)\n",
    "        context_vector = self.attention(gru_out)\n",
    "        combined_features = torch.cat((context_vector, static), dim=1)\n",
    "        output = self.classifier(combined_features)\n",
    "        return output\n",
    "print(\"Model defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Evaluation Loop\n",
      "Training function defined.\n"
     ]
    }
   ],
   "source": [
    "# 6. Training and Evaluation Loop\n",
    "print(\"Training and Evaluation Loop\")\n",
    "def train_model(model, train_loader, val_loader, epochs, learning_rate, pos_weight):\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    best_f1 = -1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\"):\n",
    "            sequences = batch['sequence'].to(DEVICE)\n",
    "            statics = batch['static'].to(DEVICE)\n",
    "            targets = batch['target'].to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences, statics)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Evaluation with threshold optimization\n",
    "        model.eval()\n",
    "        all_preds_proba, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                sequences, statics, targets = batch['sequence'].to(DEVICE), batch['static'].to(DEVICE), batch['target'].to(DEVICE)\n",
    "                outputs = model(sequences, statics)\n",
    "                all_preds_proba.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "        all_preds_proba = np.concatenate(all_preds_proba).flatten()\n",
    "        all_targets = np.concatenate(all_targets).flatten()\n",
    "        \n",
    "        thresholds = np.linspace(0.01, 0.99, 100)\n",
    "        f1_values = [f1_score(all_targets, (all_preds_proba > t).astype(int)) for t in thresholds]\n",
    "        best_f1_epoch = np.max(f1_values)\n",
    "        best_threshold_epoch = thresholds[np.argmax(f1_values)]\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val F1: {best_f1_epoch:.4f} at Threshold: {best_threshold_epoch:.2f}\")\n",
    "        \n",
    "        if best_f1_epoch > best_f1:\n",
    "            best_f1 = best_f1_epoch\n",
    "            print(f\"New best F1 score: {best_f1:.4f}. Saving model...\")\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            \n",
    "    return best_f1\n",
    "print(\"Training function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a3259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Pipeline\n"
     ]
    }
   ],
   "source": [
    "# 7. Run Pipeline\n",
    "print(\"Run Pipeline\")\n",
    "# Hyperparameters\n",
    "INPUT_SIZE = 9    \n",
    "STATIC_SIZE = 2   \n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.4\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "pos_count = train_meta['target'].sum()\n",
    "neg_count = len(train_meta) - pos_count\n",
    "pos_weight = torch.tensor([neg_count / pos_count], device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0832c217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.3139 | Val F1: 0.1114 at Threshold: 0.51\n",
      "New best F1 score: 0.1114. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 22.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 1.3112 | Val F1: 0.1090 at Threshold: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 21.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 1.3097 | Val F1: 0.1022 at Threshold: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 22.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 1.3060 | Val F1: 0.0984 at Threshold: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 22.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 1.3059 | Val F1: 0.1006 at Threshold: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 22.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 1.3984 | Val F1: 0.1029 at Threshold: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 21.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 1.2998 | Val F1: 0.1026 at Threshold: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 22.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 1.3043 | Val F1: 0.1002 at Threshold: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 22.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 1.2950 | Val F1: 0.0990 at Threshold: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 21.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 1.2964 | Val F1: 0.1018 at Threshold: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 22.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 1.2917 | Val F1: 0.1021 at Threshold: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 21.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 1.2876 | Val F1: 0.1034 at Threshold: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 1.2852 | Val F1: 0.1014 at Threshold: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 21.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 1.2915 | Val F1: 0.1022 at Threshold: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 [Training]: 100%|██████████| 77/77 [00:03<00:00, 22.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 1.3638 | Val F1: 0.1007 at Threshold: 0.50\n",
      "\n",
      "--- Training Finished ---\n",
      "Best validation F1 score achieved: 0.1114\n"
     ]
    }
   ],
   "source": [
    "model = GRUClassifier(INPUT_SIZE, STATIC_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT).to(DEVICE)\n",
    "final_f1 = train_model(model, train_loader, val_loader, EPOCHS, LEARNING_RATE, pos_weight)\n",
    "\n",
    "print(f\"\\n--- Training Finished ---\")\n",
    "print(f\"Best validation F1 score achieved: {final_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8797d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
