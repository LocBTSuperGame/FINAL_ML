{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14010596,"sourceType":"datasetVersion","datasetId":8925232}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"494b9922-defb-456e-baa3-c2fec572012b","cell_type":"code","source":"import os, glob, gc, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom scipy import stats\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nSEED = 42\nnp.random.seed(SEED)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T03:03:56.789408Z","iopub.execute_input":"2025-12-23T03:03:56.789845Z","iopub.status.idle":"2025-12-23T03:04:09.567266Z","shell.execute_reply.started":"2025-12-23T03:03:56.789803Z","shell.execute_reply":"2025-12-23T03:04:09.566234Z"}},"outputs":[],"execution_count":1},{"id":"d434719c-081c-4f16-9b8b-3c46fe2b8b25","cell_type":"code","source":"# Galactic extinction coefficients (LSST bands)\nEXTINCTION_COEFFS = {\n    \"u\": 4.239,\n    \"g\": 3.303,\n    \"r\": 2.285,\n    \"i\": 1.698,\n    \"z\": 1.263,\n    \"y\": 1.088,\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T03:04:09.569281Z","iopub.execute_input":"2025-12-23T03:04:09.570292Z","iopub.status.idle":"2025-12-23T03:04:09.577056Z","shell.execute_reply.started":"2025-12-23T03:04:09.570245Z","shell.execute_reply":"2025-12-23T03:04:09.575858Z"}},"outputs":[],"execution_count":2},{"id":"6c45e38b-06a5-4ae2-b3a8-18270760e218","cell_type":"code","source":"# LSST filters (fixed order)\nFILTERS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T03:04:09.577967Z","iopub.execute_input":"2025-12-23T03:04:09.578227Z","iopub.status.idle":"2025-12-23T03:04:09.598547Z","shell.execute_reply.started":"2025-12-23T03:04:09.578204Z","shell.execute_reply":"2025-12-23T03:04:09.597230Z"}},"outputs":[],"execution_count":3},{"id":"147780cc","cell_type":"markdown","source":"# MALLORN - Lightcurve-specialized LGBM (v2)\n- Faster feature build (groupby)\n- asinh-flux + early/late slopes + cross-band peak features\n- robust submission mapping\n- Target: improve LB > 0.6 (build on your 0.578 baseline)\n","metadata":{}},{"id":"1d543bdd-96f4-4a06-a303-8879757d6763","cell_type":"code","source":"import os, glob\nimport pandas as pd\n\ndef find_file_in_kaggle_input(filename: str):\n    hits = glob.glob(f\"/kaggle/input/**/{filename}\", recursive=True)\n    if not hits:\n        raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y {filename} trong /kaggle/input. B·∫°n ki·ªÉm tra ƒë√£ add dataset competition ch∆∞a.\")\n    # ∆∞u ti√™n file n·∫±m ·ªü c·∫•p root dataset (ƒë∆∞·ªùng d·∫´n ng·∫Øn h∆°n)\n    hits = sorted(hits, key=lambda x: (x.count(\"/\"), len(x)))\n    return hits[0]\n\ntrain_log_path = find_file_in_kaggle_input(\"train_log.csv\")\ntest_log_path  = find_file_in_kaggle_input(\"test_log.csv\")\nsample_sub_path = find_file_in_kaggle_input(\"sample_submission.csv\")\n\nprint(\"train_log:\", train_log_path)\nprint(\"test_log :\", test_log_path)\nprint(\"sample   :\", sample_sub_path)\n\ntrain_log = pd.read_csv(train_log_path)\ntest_log  = pd.read_csv(test_log_path)\nsample_sub = pd.read_csv(sample_sub_path)\n\ntrain_log.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T03:04:09.599824Z","iopub.execute_input":"2025-12-23T03:04:09.600186Z","iopub.status.idle":"2025-12-23T03:04:09.940486Z","shell.execute_reply.started":"2025-12-23T03:04:09.600147Z","shell.execute_reply":"2025-12-23T03:04:09.939181Z"}},"outputs":[{"name":"stdout","text":"train_log: /kaggle/input/mallorn-dataset/train_log.csv\ntest_log : /kaggle/input/mallorn-dataset/test_log.csv\nsample   : /kaggle/input/mallorn-dataset/sample_submission.csv\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                  object_id       Z  Z_err    EBV SpecType  \\\n0  Dornhoth_fervain_onodrim  3.0490    NaN  0.110      AGN   \n1       Dornhoth_galadh_ylf  0.4324    NaN  0.058    SN II   \n2      Elrim_melethril_thul  0.4673    NaN  0.577      AGN   \n3        Ithil_tobas_rodwen  0.6946    NaN  0.012      AGN   \n4       Mirion_adar_Druadan  0.4161    NaN  0.058      AGN   \n\n                               English Translation     split  target  \n0  Trawn Folk (Dwarfs) + northern + Ents (people)   split_01       0  \n1    Trawn Folk (Dwarfs) + tree + drinking vessel   split_01       0  \n2                  Elves +  lover (fem.)  + breath  split_01       0  \n3                    moon +  roof  +  noble maiden  split_01       0  \n4            jewel, Silmaril  + father + Wild Man   split_01       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>object_id</th>\n      <th>Z</th>\n      <th>Z_err</th>\n      <th>EBV</th>\n      <th>SpecType</th>\n      <th>English Translation</th>\n      <th>split</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dornhoth_fervain_onodrim</td>\n      <td>3.0490</td>\n      <td>NaN</td>\n      <td>0.110</td>\n      <td>AGN</td>\n      <td>Trawn Folk (Dwarfs) + northern + Ents (people)</td>\n      <td>split_01</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Dornhoth_galadh_ylf</td>\n      <td>0.4324</td>\n      <td>NaN</td>\n      <td>0.058</td>\n      <td>SN II</td>\n      <td>Trawn Folk (Dwarfs) + tree + drinking vessel</td>\n      <td>split_01</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Elrim_melethril_thul</td>\n      <td>0.4673</td>\n      <td>NaN</td>\n      <td>0.577</td>\n      <td>AGN</td>\n      <td>Elves +  lover (fem.)  + breath</td>\n      <td>split_01</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ithil_tobas_rodwen</td>\n      <td>0.6946</td>\n      <td>NaN</td>\n      <td>0.012</td>\n      <td>AGN</td>\n      <td>moon +  roof  +  noble maiden</td>\n      <td>split_01</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Mirion_adar_Druadan</td>\n      <td>0.4161</td>\n      <td>NaN</td>\n      <td>0.058</td>\n      <td>AGN</td>\n      <td>jewel, Silmaril  + father + Wild Man</td>\n      <td>split_01</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"id":"833d9689-8432-4e2e-9abd-3667de284928","cell_type":"code","source":"def find_split_root():\n    candidates = glob.glob(\"/kaggle/input/*\")\n    for c in candidates:\n        if os.path.isdir(c) and len(glob.glob(os.path.join(c, \"split_*\"))) >= 10:\n            return c\n    raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c ch·ª©a split_* trong /kaggle/input. Ki·ªÉm tra dataset ƒë√£ add ƒë√∫ng competition.\")\n\nSPLIT_ROOT = find_split_root()\nprint(\"SPLIT_ROOT:\", SPLIT_ROOT)\nprint(\"Example splits:\", sorted(glob.glob(os.path.join(SPLIT_ROOT, \"split_*\")))[:3])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T03:04:09.942883Z","iopub.execute_input":"2025-12-23T03:04:09.943188Z","iopub.status.idle":"2025-12-23T03:04:09.953020Z","shell.execute_reply.started":"2025-12-23T03:04:09.943159Z","shell.execute_reply":"2025-12-23T03:04:09.952066Z"}},"outputs":[{"name":"stdout","text":"SPLIT_ROOT: /kaggle/input/mallorn-dataset\nExample splits: ['/kaggle/input/mallorn-dataset/split_01', '/kaggle/input/mallorn-dataset/split_02', '/kaggle/input/mallorn-dataset/split_03']\n","output_type":"stream"}],"execution_count":5},{"id":"f2384667","cell_type":"code","source":"# Load logs\ntrain_log = pd.read_csv(train_log_path)\ntest_log  = pd.read_csv(test_log_path)\nsample_sub = pd.read_csv(sample_sub_path)\n\n\n# Add split_id numeric (useful feature)\ntrain_log['split_id'] = train_log['split'].str.extract(r'(\\d+)').astype(int)\ntest_log['split_id']  = test_log['split'].str.extract(r'(\\d+)').astype(int)\n\nprint(f\"Train objects: {len(train_log)} | Pos(TDE): {train_log['target'].sum()} ({train_log['target'].mean()*100:.2f}%)\")\nprint(f\"Test objects:  {len(test_log)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T03:04:09.954091Z","iopub.execute_input":"2025-12-23T03:04:09.954419Z","iopub.status.idle":"2025-12-23T03:04:10.172910Z","shell.execute_reply.started":"2025-12-23T03:04:09.954371Z","shell.execute_reply":"2025-12-23T03:04:10.171705Z"}},"outputs":[{"name":"stdout","text":"Train objects: 3043 | Pos(TDE): 148 (4.86%)\nTest objects:  7135\n","output_type":"stream"}],"execution_count":6},{"id":"ed35ed5b","cell_type":"code","source":"import glob, os\n\ndef find_split_root():\n    candidates = glob.glob(\"/kaggle/input/*\")\n    for c in candidates:\n        if os.path.isdir(c) and len(glob.glob(os.path.join(c, \"split_*\"))) >= 10:\n            return c\n    raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y split_* trong /kaggle/input. Ki·ªÉm tra ƒë√£ Add competition data ch∆∞a.\")\n\nSPLIT_ROOT = find_split_root()\nprint(\"SPLIT_ROOT =\", SPLIT_ROOT)\n\n# D√πng SPLIT_ROOT thay cho DATA_PATH trong ph·∫ßn load lightcurves\nDATA_PATH = SPLIT_ROOT\n\n# Load all lightcurves\ntrain_lc_list, test_lc_list = [], []\nfor i in tqdm(range(1, 21), desc=\"Loading splits\"):\n    split_folder = f\"split_{i:02d}\"\n    tr_path = os.path.join(DATA_PATH, split_folder, \"train_full_lightcurves.csv\")\n    te_path = os.path.join(DATA_PATH, split_folder, \"test_full_lightcurves.csv\")\n    if os.path.exists(tr_path):\n        train_lc_list.append(pd.read_csv(tr_path))\n    if os.path.exists(te_path):\n        test_lc_list.append(pd.read_csv(te_path))\n\ntrain_lc = pd.concat(train_lc_list, ignore_index=True).dropna(subset=['Flux'])\ntest_lc  = pd.concat(test_lc_list,  ignore_index=True).dropna(subset=['Flux'])\ndel train_lc_list, test_lc_list\ngc.collect()\n\nprint(f\"Train LC points: {len(train_lc):,} | objects: {train_lc['object_id'].nunique()}\")\nprint(f\"Test  LC points: {len(test_lc):,}  | objects: {test_lc['object_id'].nunique()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T03:04:10.174082Z","iopub.execute_input":"2025-12-23T03:04:10.174348Z","iopub.status.idle":"2025-12-23T03:04:12.904675Z","shell.execute_reply.started":"2025-12-23T03:04:10.174324Z","shell.execute_reply":"2025-12-23T03:04:12.903223Z"}},"outputs":[{"name":"stdout","text":"SPLIT_ROOT = /kaggle/input/mallorn-dataset\n","output_type":"stream"},{"name":"stderr","text":"Loading splits: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  8.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train LC points: 478,493 | objects: 3043\nTest  LC points: 1,143,103  | objects: 7135\n","output_type":"stream"}],"execution_count":7},{"id":"4cacd323","cell_type":"code","source":"def apply_extinction_vec(flux: np.ndarray, ebv: float, filt_arr: np.ndarray) -> np.ndarray:\n    # flux_corr = flux * 10^(0.4 * R_lambda * EBV)\n    if ebv <= 0:\n        return flux\n    coeff = np.vectorize(EXTINCTION_COEFFS.get)(filt_arr)\n    coeff = np.where(pd.isna(coeff), 0.0, coeff)\n    return flux * np.power(10.0, 0.4 * coeff * ebv)\n\ndef stat_feats(x: np.ndarray, prefix: str) -> dict:\n    f = {}\n    if x is None or len(x) < 3:\n        return f\n    x = x.astype(float)\n    f[prefix+'mean'] = np.mean(x)\n    f[prefix+'std']  = np.std(x)\n    f[prefix+'median'] = np.median(x)\n    f[prefix+'min']  = np.min(x)\n    f[prefix+'max']  = np.max(x)\n    f[prefix+'range'] = f[prefix+'max'] - f[prefix+'min']\n    for p in (5,10,25,75,90,95):\n        f[f\"{prefix}p{p}\"] = np.percentile(x, p)\n    f[prefix+'iqr'] = f[prefix+'p75'] - f[prefix+'p25']\n    if len(x) > 4:\n        f[prefix+'skew'] = stats.skew(x)\n        f[prefix+'kurt'] = stats.kurtosis(x)\n    else:\n        f[prefix+'skew'] = 0.0\n        f[prefix+'kurt'] = 0.0\n    mad = np.median(np.abs(x - f[prefix+'median']))\n    f[prefix+'mad'] = mad\n    f[prefix+'robust_std'] = 1.4826 * mad\n    # amplitude using top/bottom 5%\n    xsort = np.sort(x)\n    k = max(1, int(0.05*len(xsort)))\n    f[prefix+'amplitude'] = np.mean(xsort[-k:]) - np.mean(xsort[:k])\n    return f\n\ndef temporal_feats(t: np.ndarray, x: np.ndarray, prefix: str) -> dict:\n    f={}\n    if len(t) < 5:\n        return f\n    idx = np.argsort(t)\n    t = t[idx]; x = x[idx]\n    dt = np.diff(t)\n    f[prefix+'duration'] = t[-1]-t[0]\n    f[prefix+'cad_mean'] = np.mean(dt)\n    f[prefix+'cad_std']  = np.std(dt)\n    f[prefix+'cad_min']  = np.min(dt)\n    f[prefix+'cad_max']  = np.max(dt)\n    # peak\n    pk = int(np.argmax(x))\n    f[prefix+'t_peak'] = t[pk]\n    f[prefix+'time_to_peak'] = t[pk]-t[0]\n    f[prefix+'peak_pos'] = f[prefix+'time_to_peak']/(f[prefix+'duration']+1e-9)\n    # rise/decay\n    if pk>0:\n        f[prefix+'rise_rate'] = (x[pk]-x[0])/(t[pk]-t[0]+1e-9)\n        f[prefix+'rise_flux'] = (x[pk]-x[0])\n    else:\n        f[prefix+'rise_rate']=0.0; f[prefix+'rise_flux']=0.0\n    if pk < len(x)-1:\n        f[prefix+'decay_rate'] = (x[pk]-x[-1])/(t[-1]-t[pk]+1e-9)\n        f[prefix+'decay_flux'] = (x[pk]-x[-1])\n    else:\n        f[prefix+'decay_rate']=0.0; f[prefix+'decay_flux']=0.0\n    f[prefix+'rise_decay_ratio'] = f[prefix+'rise_rate']/(f[prefix+'decay_rate']+1e-9)\n\n    # early/late slopes (20% window)\n    dur = f[prefix+'duration'] + 1e-9\n    tn = (t - t[0]) / dur\n    early = tn <= 0.2\n    late  = tn >= 0.8\n    if early.sum() >= 3:\n        try:\n            f[prefix+'slope_early'] = stats.linregress(t[early], x[early]).slope\n            f[prefix+'mean_early']  = np.mean(x[early])\n        except:\n            f[prefix+'slope_early'] = 0.0; f[prefix+'mean_early']=np.mean(x[early])\n    else:\n        f[prefix+'slope_early']=0.0; f[prefix+'mean_early']=np.mean(x[:max(1,len(x)//5)])\n    if late.sum() >= 3:\n        try:\n            f[prefix+'slope_late']  = stats.linregress(t[late], x[late]).slope\n            f[prefix+'mean_late']   = np.mean(x[late])\n        except:\n            f[prefix+'slope_late']=0.0; f[prefix+'mean_late']=np.mean(x[late])\n    else:\n        f[prefix+'slope_late']=0.0; f[prefix+'mean_late']=np.mean(x[-max(1,len(x)//5):])\n    f[prefix+'early_late_diff'] = f[prefix+'mean_late'] - f[prefix+'mean_early']\n\n    # gradients\n    g = np.diff(x)/(dt+1e-9)\n    f[prefix+'grad_mean'] = np.mean(g)\n    f[prefix+'grad_std']  = np.std(g)\n    f[prefix+'grad_max']  = np.max(g)\n    f[prefix+'grad_min']  = np.min(g)\n    return f\n\ndef build_features(lc_df: pd.DataFrame, log_df: pd.DataFrame, is_train: bool) -> pd.DataFrame:\n    # make meta map\n    meta_cols = ['Z','Z_err','EBV','split_id'] + (['target'] if is_train else [])\n    meta = log_df.set_index('object_id')[meta_cols]\n\n    feats = []\n    for oid, obj in tqdm(lc_df.groupby('object_id', sort=False), total=lc_df['object_id'].nunique(), desc=\"Feature build\"):\n        if oid not in meta.index:\n            continue\n        m = meta.loc[oid]\n        z = float(m['Z']) if pd.notna(m['Z']) else 0.0\n        ebv = float(m['EBV']) if pd.notna(m['EBV']) else 0.0\n        zid = int(m['split_id'])\n\n        f = {'object_id': oid, 'Z': z, 'Z_err': float(m['Z_err']) if 'Z_err' in m else np.nan, 'EBV': ebv, 'split_id': zid}\n        f['logZ'] = np.log10(z+0.01)\n        f['Z_EBV'] = z*ebv\n\n        # extinction corrected flux\n        flux = obj['Flux'].values.astype(float)\n        ferr = obj['Flux_err'].values.astype(float)\n        band = obj['Filter'].values.astype(str)\n        time = obj['Time (MJD)'].values.astype(float)\n\n        fluxc = apply_extinction_vec(flux, ebv, band)\n        # asinh transform (handles negatives)\n        flux_asinh = np.arcsinh(fluxc)\n\n        # overall\n        f.update(stat_feats(fluxc, 'all_'))\n        f.update(stat_feats(flux_asinh, 'all_asinh_'))\n        f.update(temporal_feats(time, fluxc, 'all_'))\n\n        # per band\n        per_band = {}\n        for b in FILTERS:\n            mask = (band == b)\n            if mask.sum() < 5:\n                continue\n            tb = time[mask]; xb = fluxc[mask]\n            xab = np.arcsinh(xb)\n            f.update(stat_feats(xb, f'{b}_'))\n            f.update(stat_feats(xab, f'{b}_asinh_'))\n            f.update(temporal_feats(tb, xb, f'{b}_'))\n            per_band[b] = {\n                'mean': np.mean(xb),\n                'max': np.max(xb),\n                't_peak': tb[np.argmax(xb)]\n            }\n\n        # cross-band peak-time and peak-ratio features\n        if 'g' in per_band and 'r' in per_band:\n            f['gr_peak_ratio'] = per_band['g']['max'] / (per_band['r']['max'] + 1e-9)\n            f['gr_tpeak_diff'] = per_band['g']['t_peak'] - per_band['r']['t_peak']\n            f['gr_mean_diff']  = per_band['g']['mean'] - per_band['r']['mean']\n        if 'u' in per_band and 'g' in per_band:\n            f['ug_mean_diff'] = per_band['u']['mean'] - per_band['g']['mean']\n\n        # counts\n        f['n_filters'] = len(set(band.tolist()))\n        f['total_obs'] = len(obj)\n\n        if is_train:\n            f['target'] = int(m['target'])\n        feats.append(f)\n    return pd.DataFrame(feats)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T03:04:12.906142Z","iopub.execute_input":"2025-12-23T03:04:12.906437Z","iopub.status.idle":"2025-12-23T03:04:12.940894Z","shell.execute_reply.started":"2025-12-23T03:04:12.906400Z","shell.execute_reply":"2025-12-23T03:04:12.939636Z"}},"outputs":[],"execution_count":8},{"id":"0039fd79","cell_type":"code","source":"# Build feature tables\ntrain_features = build_features(train_lc, train_log, is_train=True)\ntest_features  = build_features(test_lc,  test_log,  is_train=False)\n\nprint(\"train_features:\", train_features.shape, \" test_features:\", test_features.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T03:04:12.942093Z","iopub.execute_input":"2025-12-23T03:04:12.942470Z","iopub.status.idle":"2025-12-23T03:09:59.669325Z","shell.execute_reply.started":"2025-12-23T03:04:12.942432Z","shell.execute_reply":"2025-12-23T03:09:59.668169Z"}},"outputs":[{"name":"stderr","text":"Feature build: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3043/3043 [01:42<00:00, 29.68it/s]\nFeature build: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7135/7135 [04:02<00:00, 29.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"train_features: (3043, 420)  test_features: (7135, 419)\n","output_type":"stream"}],"execution_count":9},{"id":"3ec30cc5","cell_type":"code","source":"# Prepare X/y\ntrain_features = train_features.drop_duplicates('object_id')\ntest_features  = test_features.drop_duplicates('object_id')\n\ny = train_features['target'].astype(int).values\n\n# IMPORTANT: Do NOT use split_id as a model feature.\n# Use it only as a GROUP for cross-validation to avoid leakage.\ngroups = train_features['split_id'].astype(int).values\n\ndrop_cols = ['object_id','target','split_id']\nX = train_features.drop(columns=drop_cols)\nX_test = test_features.drop(columns=['object_id','split_id'])\n\n# clean inf/nan\nX = X.replace([np.inf,-np.inf], np.nan).fillna(-999)\nX_test = X_test.replace([np.inf,-np.inf], np.nan).fillna(-999)\n\npos = int(y.sum()); neg = int(len(y)-pos)\nspw = neg / max(pos,1)\nprint(\"pos:\",pos,\"neg:\",neg,\"scale_pos_weight:\",spw)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T03:09:59.670734Z","iopub.execute_input":"2025-12-23T03:09:59.671065Z","iopub.status.idle":"2025-12-23T03:09:59.748121Z","shell.execute_reply.started":"2025-12-23T03:09:59.671037Z","shell.execute_reply":"2025-12-23T03:09:59.747236Z"}},"outputs":[{"name":"stdout","text":"pos: 148 neg: 2895 scale_pos_weight: 19.56081081081081\n","output_type":"stream"}],"execution_count":10},{"id":"c283e2d9","cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nimport lightgbm as lgb\nimport numpy as np\n\n# =========================\n# CONFIG\n# =========================\nN_SPLITS = 5\nSEEDS = [42, 7, 2025]   # 3-seed bagging (·ªïn ƒë·ªãnh + v·∫´n <30 ph√∫t)\n\nlgb_params = dict(\n    objective=\"binary\",\n    boosting_type=\"gbdt\",\n    metric=\"auc\",\n    learning_rate=0.03,\n    n_estimators=12000,\n    num_leaves=127,\n    max_depth=-1,\n    min_child_samples=20,\n    feature_fraction=0.80,\n    bagging_fraction=0.80,\n    bagging_freq=1,\n    reg_alpha=0.0,\n    reg_lambda=1.0,\n    scale_pos_weight=spw,\n    n_jobs=-1,\n    verbose=-1,\n)\n\n# =========================\n# GROUP K-FOLD\n# =========================\ngkf = GroupKFold(n_splits=N_SPLITS)\nfolds = list(gkf.split(X, y, groups=groups))\nprint(\"Fold sizes:\", [len(va) for _, va in folds])\n\n# =========================\n# TRAIN (OOF + TEST)\n# =========================\noof_sum = np.zeros(len(X), dtype=float)\noof_cnt = np.zeros(len(X), dtype=int)\ntest_prob = np.zeros(len(X_test), dtype=float)\n\nfor seed in SEEDS:\n    print(f\"\\n=== Seed {seed} ===\")\n    for fold, (tr_idx, va_idx) in enumerate(folds):\n        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n        X_va, y_va = X.iloc[va_idx], y[va_idx]\n\n        params = dict(lgb_params)\n        params[\"random_state\"] = seed + fold\n\n        model = lgb.LGBMClassifier(**params)\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            callbacks=[lgb.early_stopping(400, verbose=False)]\n        )\n\n        # OOF\n        vp = model.predict_proba(X_va)[:, 1]\n        oof_sum[va_idx] += vp\n        oof_cnt[va_idx] += 1\n\n        # TEST\n        test_prob += model.predict_proba(X_test)[:, 1] / (len(SEEDS) * N_SPLITS)\n\n# =========================\n# FINAL OOF\n# =========================\noof_avg = oof_sum / np.maximum(oof_cnt, 1)\n\n# =========================\n# THRESHOLD ‚Äì STEP 1: OOF F1\n# =========================\nths = np.linspace(0.05, 0.95, 200)\nf1s = np.array([f1_score(y, (oof_avg >= t).astype(int)) for t in ths])\n\nbest_idx = int(np.argmax(f1s))\nbest_t = float(ths[best_idx])\nbest_f1 = float(f1s[best_idx])\n\nprint(\"OOF best F1:\", best_f1)\nprint(\"best_t:\", best_t)\nprint(\"OOF precision:\", precision_score(y, (oof_avg >= best_t).astype(int), zero_division=0))\nprint(\"OOF recall:\", recall_score(y, (oof_avg >= best_t).astype(int), zero_division=0))\n\n\n# =========================\n# THRESHOLD ‚Äì STEP 2: LB-ORIENTED POS-RATE\n# =========================\ntrain_pos_rate = float(y.mean())\ntarget_rate = min(0.25, train_pos_rate * 1.20)   # v√†ng cho MALLORN\n\ncand_t = float(np.quantile(oof_avg, 1.0 - target_rate))\n\n# blend threshold (·ªïn ƒë·ªãnh h∆°n)\nt_final = 0.7 * cand_t + 0.3 * best_t\n\nprint(\"\\ntrain_pos_rate:\", train_pos_rate)\nprint(\"target_rate:\", target_rate)\nprint(\"cand_t:\", cand_t)\nprint(\"FINAL threshold:\", t_final)\n\nprint(\"Expected submission pos-rate:\",\n      float((test_prob >= t_final).mean()))\n\n\n# =========================\n# MAKE SUBMISSION\n# =========================\npred_bin = (test_prob >= t_final).astype(int)\n\npred_dict = dict(zip(test_features[\"object_id\"], pred_bin))\nsub_final = sample_sub.copy()\nsub_final[\"prediction\"] = sub_final[\"object_id\"].map(pred_dict).fillna(0).astype(int)\n\nprint(\"Final submission pos-rate:\", sub_final[\"prediction\"].mean())\n\nsub_final.to_csv(\"submission_lgbm_lightcurve_v3_plus.csv\", index=False)\nprint(\"Saved submission_lgbm_lightcurve_v3_plus.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T03:09:59.749233Z","iopub.execute_input":"2025-12-23T03:09:59.749561Z","iopub.status.idle":"2025-12-23T03:13:13.195820Z","shell.execute_reply.started":"2025-12-23T03:09:59.749525Z","shell.execute_reply":"2025-12-23T03:13:13.194888Z"}},"outputs":[{"name":"stdout","text":"Fold sizes: [613, 600, 611, 608, 611]\n\n=== Seed 42 ===\n\n=== Seed 7 ===\n\n=== Seed 2025 ===\nOOF best F1: 0.4336569579288026\nbest_t: 0.08165829145728642\nOOF precision: 0.4161490683229814\nOOF recall: 0.4527027027027027\n\ntrain_pos_rate: 0.04863621426224121\ntarget_rate: 0.05836345711468945\ncand_t: 0.0707101690343783\nFINAL threshold: 0.07399460576125073\nExpected submission pos-rate: 0.06601261387526279\nFinal submission pos-rate: 0.06601261387526279\nSaved submission_lgbm_lightcurve_v3_plus.csv\n","output_type":"stream"}],"execution_count":11},{"id":"a62c7527-558b-4086-a2e8-1a49b54e26cd","cell_type":"code","source":"# =========================\n# MICRO RATE SWEEP (ƒÇN ƒêI·ªÇM)\n# =========================\nN_TEST = len(test_prob)\n\nfor RATE in [0.045, 0.047, 0.048, 0.05, 0.052, 0.055, 0.058]:\n    k = int(RATE * N_TEST)\n    thr = np.partition(test_prob, -k)[-k]\n    pr = (test_prob >= thr).mean()\n    print(f\"RATE={RATE:.3f} -> pos_rate={pr:.4f}, thr={thr:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T03:13:13.196941Z","iopub.execute_input":"2025-12-23T03:13:13.197230Z","iopub.status.idle":"2025-12-23T03:13:13.205034Z","shell.execute_reply.started":"2025-12-23T03:13:13.197198Z","shell.execute_reply":"2025-12-23T03:13:13.204125Z"}},"outputs":[{"name":"stdout","text":"RATE=0.045 -> pos_rate=0.0450, thr=0.1796\nRATE=0.047 -> pos_rate=0.0470, thr=0.1653\nRATE=0.048 -> pos_rate=0.0479, thr=0.1604\nRATE=0.050 -> pos_rate=0.0499, thr=0.1494\nRATE=0.052 -> pos_rate=0.0520, thr=0.1356\nRATE=0.055 -> pos_rate=0.0549, thr=0.1210\nRATE=0.058 -> pos_rate=0.0579, thr=0.1043\n","output_type":"stream"}],"execution_count":12},{"id":"3897f45d","cell_type":"code","source":"# =========================\n# RANKING-BASED SUBMISSION (ƒÇN ƒêI·ªÇM)\n# =========================\n\n# s·ªë l∆∞·ª£ng TDE d·ª± ƒëo√°n ‚Äì ch·ªânh ƒë√∫ng l√† l√™n ƒëi·ªÉm\n# v√πng v√†ng cho MALLORN: 4% ‚Äì 7% t·ªïng test\nN_TEST = len(test_prob)\n\nfor rate in [0.04, 0.05, 0.06, 0.07]:\n    k = int(rate * N_TEST)\n    thr = np.partition(test_prob, -k)[-k]\n    pr = (test_prob >= thr).mean()\n    print(f\"rate={rate:.2f} -> threshold={thr:.4f}, pos_rate={pr:.4f}\")\n\n# üëâ CH·ªåN rate = 0.05 L√Ä KHUY·∫æN NGH·ªä ƒê·∫¶U TI√äN\nRATE = 0.05\nk = int(RATE * N_TEST)\nthr_rank = np.partition(test_prob, -k)[-k]\n\nprint(\"\\nFINAL ranking threshold:\", thr_rank)\n\npred = (test_prob >= thr_rank).astype(int)\n\npred_dict = dict(zip(test_features[\"object_id\"], pred))\nsubmission = sample_sub.copy()\nsubmission[\"prediction\"] = submission[\"object_id\"].map(pred_dict).fillna(0).astype(int)\n\nprint(\"Submission rows:\", len(submission),\n      \"pos rate:\", float(submission[\"prediction\"].mean()))\n\nsubmission.to_csv(\"submission_lgbm_lightcurve_v5_rank.csv\", index=False)\nprint(\"Saved: submission_lgbm_lightcurve_v5_rank.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T03:13:13.206727Z","iopub.execute_input":"2025-12-23T03:13:13.207013Z","iopub.status.idle":"2025-12-23T03:13:13.244826Z","shell.execute_reply.started":"2025-12-23T03:13:13.206985Z","shell.execute_reply":"2025-12-23T03:13:13.243859Z"}},"outputs":[{"name":"stdout","text":"rate=0.04 -> threshold=0.2362, pos_rate=0.0399\nrate=0.05 -> threshold=0.1494, pos_rate=0.0499\nrate=0.06 -> threshold=0.0925, pos_rate=0.0600\nrate=0.07 -> threshold=0.0659, pos_rate=0.0699\n\nFINAL ranking threshold: 0.14944622773289634\nSubmission rows: 7135 pos rate: 0.04989488437281009\nSaved: submission_lgbm_lightcurve_v5_rank.csv\n","output_type":"stream"}],"execution_count":13}]}